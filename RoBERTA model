"""@inproceedings{perez-almendros2020dontpatronizeme,
  title={Donâ€™t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities},
  author={Perez-Almendros, Carla and Espinosa-Anke, Luis and Schockaert, Steven},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={5891--5902},
  year={2020}
}
"""

# -*- coding: utf-8 -*-
"""Submitted RoBERTa baseline train-dev dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18gF0cpRVUAticWDdRKdKwTkcxrQVE9rO

# Main imports and code
"""

# check which gpu we're using
!nvidia-smi

!pip install simpletransformers
!pip install tensorboardx

from simpletransformers.classification import ClassificationModel, ClassificationArgs, MultiLabelClassificationModel, MultiLabelClassificationArgs
from urllib import request
import pandas as pd
import logging
import torch
from collections import Counter
from ast import literal_eval
import os

# prepare logger
logging.basicConfig(level=logging.INFO)

transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

# check gpu
cuda_available = torch.cuda.is_available()

print('Cuda available? ',cuda_available)

if cuda_available:
  import tensorflow as tf
  # Get the GPU device name.
  device_name = tf.test.gpu_device_name()
  # The device name should look like the following:
  if device_name == '/device:GPU:0':
      print('Found GPU at: {}'.format(device_name))
  else:
      raise SystemError('GPU device not found')

"""# Fetch Don't Patronize Me! data manager module"""

module_url = f"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py"
module_name = module_url.split('/')[-1]
print(f'Fetching {module_url}')
#with open("file_1.txt") as f1, open("file_2.txt") as f2
with request.urlopen(module_url) as f, open(module_name,'w') as outf:
  a = f.read()
  outf.write(a.decode('utf-8'))

# helper function to save predictions to an output file
def labels2file(p, outf_path):
	with open(outf_path,'w') as outf:
		for pi in p:
			outf.write(','.join([str(k) for k in pi])+'\n')

from dont_patronize_me import DontPatronizeMe

dpm = DontPatronizeMe('.', '.')

dpm.load_task1()
dpm.load_task2(return_one_hot=True)

trids = pd.read_csv('train_semeval_parids-labels.csv')
teids = pd.read_csv('dev_semeval_parids-labels.csv')

trids.head()

trids.par_id = trids.par_id.astype(str)
teids.par_id = teids.par_id.astype(str)



"""Build Directory for predictions"""

# first, we need to create the res/ and ref/ folders, which the evaluator expects
!mkdir ref res

"""# Rebuild training set (Task 1)"""

rows = [] # will contain par_id, label and text
for idx in range(len(trids)):  
  parid = trids.par_id[idx]
  #print(parid)
  # select row from original dataset to retrieve `text` and binary label
  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]
  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]
  rows.append({
      'par_id':parid,
      'text':text,
      'label':label
  })

len(rows)

trdf1 = pd.DataFrame(rows)

"""# Rebuild test set (Task 1)"""

rows = [] # will contain par_id, label and text
for idx in range(len(teids)):  
  parid = teids.par_id[idx]
  #print(parid)
  # select row from original dataset
  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]
  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]
  rows.append({
      'par_id':parid,
      'text':text,
      'label':label
  })

len(rows)

tedf1 = pd.DataFrame(rows)
tedf1

"""# RoBERTa Baseline for Task 1"""

# downsample negative instances
pcldf = trdf1[trdf1.label==1]
npos = len(pcldf)

training_set1 = pd.concat([pcldf,trdf1[trdf1.label==0][:npos*2]])

print(len(tedf1))
gold = tedf1.label.tolist()

training_set1

task1_model_args = ClassificationArgs(num_train_epochs=5, 
                                      eval_batch_size=32,
                                      # train_batch_size=16,
                                      learning_rate= 3e-5,
                                      # weight_decay= 0.0,
                                      # adam_epsilon= 1e-08,
                                      # max_grad_norm= 1.0,
                                      # per_device_train_batch_size= 32,
                                      train_batch_size= 16,
                                      gradient_accumulation_steps= 2,
                                      # warmup_steps= 3500,
                                      # seed= 42,
                                      no_save=True, 
                                      no_cache=True, 
                                      overwrite_output_dir=True)
task1_model = ClassificationModel("roberta", 
                                  'roberta-base', 
                                  args = task1_model_args, 
                                  num_labels=2, 
                                  use_cuda=cuda_available)
# train model
task1_model.train_model(training_set1[['text', 'label']])
# run predictions
preds_task1, _ = task1_model.predict(tedf1.text.tolist())

Counter(preds_task1)

labels2file([[k] for k in preds_task1], os.path.join('res/', 'task1.txt'))

import sys
import os
import os.path
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
import numpy as np

#Evaluation

module_url = f"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/evaluation.py"
module_name = module_url.split('/')[-1]
print(f'Fetching {module_url}')
#with open("file_1.txt") as f1, open("file_2.txt") as f2
with request.urlopen(module_url) as f, open(module_name,'w') as outf:
  a = f.read()
  outf.write(a.decode('utf-8'))

labels2file(dpm.train_task1_df.label.apply(lambda x:[x]).tolist(), os.path.join('ref/', 'task1.txt'))

labels2file(dpm.train_task2_df.label.tolist(), os.path.join('ref/', 'task2.txt'))

!python3 evaluation.py . .

# evaluating on task 1
# if 'task1.txt' in files:
task1_res = []
task1_gold = []
with open(os.path.join('res/task1.txt')) as f:
    for line in f:
        task1_res.append(int(line.strip()))
# with open(os.path.join('ref/task1.txt')) as f:
#     counter_one = 0
#     counter = 2094
#     for line in f:
#         counter_one = counter_one +1
#         if counter_one <= counter:
#           task1_gold.append(int(line.strip()))

# task 1 scores
print(len(task1_gold))
t1p = precision_score(gold, task1_res)
t1r = recall_score(gold, task1_res)
t1f = f1_score(gold, task1_res)
# task1
print("Task One Scores")
print("")
print("")
print("precision: ")
print(t1p)
print("recall: " )
print(t1r)
print("F1: ")
print(t1f)
# outf.write('task1_precision:'+str(t1p)+'\n')
# outf.write('task1_recall:'+str(t1r)+'\n')
# outf.write('task1_f1:'+str(t1f)+'\n')    
# outf.close()

with open('ref/task1.txt', 'r') as goldFile:
  counter_one = 0
  counter = 2094
  d = goldFile.readlines()
  b = []
for line in d:
    counter_one = counter_one +1

    if counter_one <= counter:
     b.append(line)
      # del d[counter_one]
print(len(b))

task1_res = []
task1_gold = []
task2_res = []
task2_gold = []

with open('res/task1.txt', 'r') as resultFile_task_one:
  for line in resultFile_task_one:
    task1_res.append(int(line.strip()))
task1_gold = list(map(int, b))



# task 1 scores
t1p = precision_score(task1_gold, task1_res)
t1r = recall_score(task1_gold, task1_res)
t1f = f1_score(task1_gold, task1_res)
print("Task One Scores")
print("")
print("")
print("precision: ")
print(t1p)
print("recall: " )
print(t1r)
print("F1: ")
print(t1f)

"""# Rebuild training set (Task 2)"""

rows2 = [] # will contain par_id, label and text
for idx in range(len(trids)):  
  parid = trids.par_id[idx]
  label = trids.label[idx]
  # select row from original dataset to retrieve the `text` value
  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]
  rows2.append({
      'par_id':parid,
      'text':text,
      'label':label
  })

trdf2 = pd.DataFrame(rows2)

trdf2

trdf2.label = trdf2.label.apply(literal_eval)

"""# Rebuild test set (Task 2)"""

rows2 = [] # will contain par_id, label and text
for idx in range(len(teids)):  
  parid = teids.par_id[idx]
  label = teids.label[idx]
  #print(parid)
  # select row from original dataset to access the `text` value
  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]
  rows2.append({
      'par_id':parid,
      'text':text,
      'label':label
  })

tedf2 = pd.DataFrame(rows2)

tedf2

tedf2.label = tedf2.label.apply(literal_eval)

"""# RoBERTa baseline for Task 2"""

all_negs = trdf2[trdf2.label.apply(lambda x:sum(x) == 0)]
all_pos = trdf2[trdf2.label.apply(lambda x:sum(x) > 0)]

training_set2 = pd.concat([all_pos,all_negs[:round(len(all_pos)*0.5)]])

training_set2

task2_model_args = MultiLabelClassificationArgs(num_train_epochs=20,
                                                no_save=True, 
                                                no_cache=True, 
                                                overwrite_output_dir=True,
                                                # eval_batch_size = 16,
                                                # evaluate_during_training = True,
                                                # evaluate_during_training_silent = False,
                                                # evaluate_during_training_steps = -1,
                                                # save_eval_checkpoints = False,
                                                # save_model_every_epoch = False,
                                                # learning_rate = 1e-5,
                                                # manual_seed = 4,
                                                # max_seq_length = 256,
                                                # multiprocessing_chunksize = 5000,
                                                # reprocess_input_data = True,
                                                # train_batch_size = 16,
                                                # gradient_accumulation_steps = 2,
                                                # labels_list = ["not_entailment", "entailment"],
                                                # output_dir = "default_output",
                                                # best_model_dir = "default_output/best_model",
                                                # wandb_project = "RTE - Hyperparameter Optimization",
                                                # wandb_kwargs = {"name": "default"}

                                                )
task2_model = MultiLabelClassificationModel("roberta", 
                                            'roberta-base', 
                                            num_labels=7,
                                            args = task2_model_args, 
                                            use_cuda=cuda_available)
# train model
task2_model.train_model(training_set2[['text', 'label']])
# run predictions
preds_task2, _ = task2_model.predict(tedf2.text.tolist())

labels2file(preds_task2, os.path.join('res/','task2.txt'))

import sys
import os
import os.path
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
import numpy as np

task2_res = []
task2_gold = []
with open('res/task2.txt', 'r') as resultFile_task_two:
  for line in resultFile_task_two:
    scores = [int(k) for k in line.strip().split(',')]
    task2_res.append(scores)
with open('ref/task2.txt', 'r') as goldFile_task_two:
  for line in goldFile_task_two:
    scores = [int(k) for k in line.strip().split(',')]
    task2_gold.append(scores)
    # task 2 scores
  t2u_gold_list = []
  t2u_pred_list = []

  t2s_gold_list = []
  t2s_pred_list = []

  t2p_gold_list = []
  t2p_pred_list = []

  t2a_gold_list = []
  t2a_pred_list = []

  t2m_gold_list = []
  t2m_pred_list = []

  t2c_gold_list = []
  t2c_pred_list = []

  t2t_gold_list = []
  t2t_pred_list = []

  for idx in range(len(task2_gold)):
      t2u_gold,t2s_gold,t2p_gold,t2a_gold,t2m_gold,t2c_gold,t2t_gold = task2_gold[idx]
      t2u_pred,t2s_pred,t2p_pred,t2a_pred,t2m_pred,t2c_pred,t2t_pred = task2_res[idx]

      t2u_gold_list.append(t2u_gold)
      t2u_pred_list.append(t2u_pred)

      t2s_gold_list.append(t2s_gold)
      t2s_pred_list.append(t2s_pred)

      t2p_gold_list.append(t2p_gold)
      t2p_pred_list.append(t2p_pred)

      t2a_gold_list.append(t2a_gold)
      t2a_pred_list.append(t2a_pred)

      t2m_gold_list.append(t2m_gold)
      t2m_pred_list.append(t2m_pred)

      t2c_gold_list.append(t2c_gold)
      t2c_pred_list.append(t2c_pred)

      t2t_gold_list.append(t2t_gold)
      t2t_pred_list.append(t2t_pred)

  t2u = f1_score(t2u_gold_list,t2u_pred_list)
  t2s = f1_score(t2s_gold_list, t2s_pred_list)
  t2p = f1_score(t2p_gold_list, t2p_pred_list)
  t2a = f1_score(t2a_gold_list, t2a_pred_list)
  t2m = f1_score(t2m_gold_list, t2m_pred_list)
  t2c = f1_score(t2c_gold_list, t2c_pred_list)
  t2t = f1_score(t2t_gold_list, t2t_pred_list)
  t2avg = np.mean([t2u, t2s, t2p, t2a, t2m, t2c, t2t])

print("")
print("")
print("Task Two Scores ")
print("")
print("t2u F1 Score: ")
print(t2u)
print("t2s F1 Score: ")
print(t2s)
print("t2p F1 Score: ")
print(t2p)
print("t2a F1 Score: ")
print(t2a)
print("t2m F1 Score: ")
print(t2m)
print("t2c F1 Score: ")
print(t2c)
print("t2t F1 Score: ")
print(t2t)
print("t2avg Mean: ")
print(t2avg)

"""## Prepare submission"""

!cat res/task1.txt | head -n 10

!cat res/task2.txt | head -n 10

!zip submission.zip res/task1.txt res/task2.txt

"""## Fetching Evaluation Script"""

module_url = f"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/evaluation.py"
module_name = module_url.split('/')[-1]
print(f'Fetching {module_url}')
#with open("file_1.txt") as f1, open("file_2.txt") as f2
with request.urlopen(module_url) as f, open(module_name,'w') as outf:
  a = f.read()
  outf.write(a.decode('utf-8'))

# for subtask 1 (we convert our list of labels into a list of lists to make 
# it compatible with the labels2file function)
labels2file(dpm.train_task1_df.label.apply(lambda x:[x]).tolist(), os.path.join('ref/', 'task1.txt'))
# and for subtask 2
labels2file(dpm.train_task2_df.label.tolist(), os.path.join('ref/', 'task2.txt'))

# Now, we can just call the official scorer, which takes an input_directory and an output_directory
# as arguments. In this example, both will be the root directory of this notebook.
!python3 evaluation.py . .

# The scorer generated a results file called "scores.txt". 
# We can now see the performance of a random baseline on the training set.
!cat scores.txt

"""Starting Evaluation"""

import sys
import os
import os.path
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
import numpy as np





with open('ref/task1.txt', 'r') as goldFile:
  counter_one = 0
  counter = 2094
  d = goldFile.readlines()
  b = []
for line in d:
    counter_one = counter_one +1

    if counter_one <= counter:
     b.append(line)
      # del d[counter_one]
print(len(b))

task1_res = []
task1_gold = []
task2_res = []
task2_gold = []

with open('res/task1.txt', 'r') as resultFile_task_one:
  for line in resultFile_task_one:
    task1_res.append(int(line.strip()))
task1_gold = list(map(int, b))



# task 1 scores
t1p = precision_score(task1_gold, task1_res)
t1r = recall_score(task1_gold, task1_res)
t1f = f1_score(task1_gold, task1_res)
print("Task One Scores")
print("")
print("")
print("precision: ")
print(t1p)
print("recall: " )
print(t1r)
print("F1: ")
print(t1f)
# task1
# outf.write('task1_precision:'+str(t1p)+'\n')
# outf.write('task1_recall:'+str(t1r)+'\n')
# outf.write('task1_f1:'+str(t1f)+'\n')    

#task 2 scores

with open('res/task2.txt', 'r') as resultFile_task_two:
  for line in resultFile_task_two:
    scores = [int(k) for k in line.strip().split(',')]
    task2_res.append(scores)
with open('ref/task2.txt', 'r') as goldFile_task_two:
  for line in goldFile_task_two:
    scores = [int(k) for k in line.strip().split(',')]
    task2_gold.append(scores)
t2u_gold_list = []
t2u_pred_list = []

t2s_gold_list = []
t2s_pred_list = []

t2p_gold_list = []
t2p_pred_list = []

t2a_gold_list = []
t2a_pred_list = []

t2m_gold_list = []
t2m_pred_list = []

t2c_gold_list = []
t2c_pred_list = []

t2t_gold_list = []
t2t_pred_list = []

for idx in range(len(task2_gold)):
  t2u_gold,t2s_gold,t2p_gold,t2a_gold,t2m_gold,t2c_gold,t2t_gold = task2_gold[idx]
  t2u_pred,t2s_pred,t2p_pred,t2a_pred,t2m_pred,t2c_pred,t2t_pred = task2_res[idx]

  t2u_gold_list.append(t2u_gold)
  t2u_pred_list.append(t2u_pred)

  t2s_gold_list.append(t2s_gold)
  t2s_pred_list.append(t2s_pred)

  t2p_gold_list.append(t2p_gold)
  t2p_pred_list.append(t2p_pred)

  t2a_gold_list.append(t2a_gold)
  t2a_pred_list.append(t2a_pred)

  t2m_gold_list.append(t2m_gold)
  t2m_pred_list.append(t2m_pred)

  t2c_gold_list.append(t2c_gold)
  t2c_pred_list.append(t2c_pred)

  t2t_gold_list.append(t2t_gold)
  t2t_pred_list.append(t2t_pred)

t2u = f1_score(t2u_gold_list,t2u_pred_list)
t2s = f1_score(t2s_gold_list, t2s_pred_list)
t2p = f1_score(t2p_gold_list, t2p_pred_list)
t2a = f1_score(t2a_gold_list, t2a_pred_list)
t2m = f1_score(t2m_gold_list, t2m_pred_list)
t2c = f1_score(t2c_gold_list, t2c_pred_list)
t2t = f1_score(t2t_gold_list, t2t_pred_list)
t2avg = np.mean([t2u, t2s, t2p, t2a, t2m, t2c, t2t])
print("")
print("")
print("Task Two Scores ")
print("")
print("t2u F1 Score: ")
print(t2u)
print("t2s F1 Score: ")
print(t2s)
print("t2p F1 Score: ")
print(t2p)
print("t2a F1 Score: ")
print(t2a)
print("t2m F1 Score: ")
print(t2m)
print("t2c F1 Score: ")
print(t2c)
print("t2t F1 Score: ")
print(t2t)
print("t2avg Mean: ")
print(t2avg)
